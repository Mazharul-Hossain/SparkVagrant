{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkPi\n",
    "\n",
    "We now turn to doing some lightweight Spark stuff in Scala. \n",
    "\n",
    "This is the same  [SparkPi program]([https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala) available in the Scala section of Spark examples.\n",
    "\n",
    "It computes $\\pi$ up to some digits by computing the area of the unit circle (which is $A = \\pi  r^2 = \\pi$). To do so, the unit square is repeteadly sampled (by randomly taking points from it) and the points falling within the unit circle are counted; the result is an approximation to the desired area. See the [wikipedia page](https://en.wikipedia.org/wiki/Approximations_of_%CF%80#Summing_a_circle.27s_area) for further explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As a complete program\n",
    "We could execute the complete SparkPI Scala snippet. Let's take a look at the original source:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Scala\n",
    "import scala.math.random\n",
    "import org.apache.spark._\n",
    "\n",
    "/** Computes an approximation to pi */\n",
    "object SparkPi {\n",
    "  def main(args: Array[String]) {\n",
    "    val conf = new SparkConf().setAppName(\"Spark Pi\")\n",
    "    val spark = new SparkContext(conf)\n",
    "    val slices = if (args.length > 0) args(0).toInt else 2\n",
    "    val n = 100000 * slices\n",
    "    val count = sc.parallelize(1 to n, slices).map { i =>\n",
    "      val x = random * 2 - 1\n",
    "      val y = random * 2 - 1\n",
    "      if (x*x + y*y < 1) 1 else 0\n",
    "    }.reduce(_ + _)\n",
    "    println(\"Pi is roughly \" + 4.0 * count / n)\n",
    "    spark.stop()\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be run with a small modification: the Scala kernel used in this notebook ([Toree](https://toree.incubator.apache.org/)) already provides a `SparkContext` ready for us to use, in the `sc` variable. \n",
    "So we do not need to create a new context by executing\n",
    "```Scala\n",
    "    val spark = new SparkContext(conf)\n",
    "```\n",
    " ... and indeed we should **not** do so, because the operation tries to create *another* context, and unless the kernel is running with the configuration property *spark.driver.allowMultipleContexts* set to `true`, it will fail. \n",
    " \n",
    " So let's use that already-available context, and redefine the program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.math.random\n",
    "import org.apache.spark._\n",
    "\n",
    "/** Computes an approximation to pi */\n",
    "object SparkPi {\n",
    "  def main(args: Array[String]) {\n",
    "    val conf = sc.getConf /* <--- we use the context the kernel has already created */\n",
    "    conf.setAppName( \"Spark Pi\" )\n",
    "    val slices = if (args.length > 0) args(0).toInt else 2\n",
    "    val n = 100000 * slices\n",
    "    val count = sc.parallelize(1 to n, slices).map { i =>\n",
    "      val x = random * 2 - 1\n",
    "      val y = random * 2 - 1\n",
    "      if (x*x + y*y < 1) 1 else 0\n",
    "    }.reduce(_ + _)\n",
    "    println(\"Pi is roughly \" + 4.0 * count / n)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we execute the defined program by calling it with the required arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkPi.main( Array(\"2\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As a notebook\n",
    "\n",
    "Now we do the same, but in a more notebook-friendly shape by splitting the program into cells to be computed sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Imports we need\n",
    "import scala.math.random\n",
    "import org.apache.spark._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set out application name\n",
    "val conf = sc.getConf\n",
    "conf.setAppName( \"Spark Pi\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Define how many threads we want\n",
    "val slices = 2;\n",
    "// We will sample 100K points per thread\n",
    "val n = 100000 * slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// the program\n",
    "val count = sc.parallelize(1 to n, slices).map { i =>\n",
    "      val x = random * 2 - 1\n",
    "      val y = random * 2 - 1\n",
    "      if (x*x + y*y < 1) 1 else 0\n",
    "    }.reduce(_ + _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// The result\n",
    "println(\"Pi is roughly \" + 4.0 * count / n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Python\n",
    "\n",
    "Side note: the `SPylon` kernel can also execute Python/Pyspark code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from operator import add\n",
    "print(sc.parallelize(range(1, 100)).reduce(add))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11 (SPylon)",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
